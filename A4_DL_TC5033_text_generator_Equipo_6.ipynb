{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Handrum/ML_Equipo_6/blob/main/A4_DL_TC5033_text_generator_Equipo_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">\n",
        "![Evidence 3](https://i.imgur.com/mu6ZuGT.jpg)\n",
        "\n",
        "# **Master's in Applied Artificial Intelligence**\n",
        "\n",
        "## **TC 5033**\n",
        "## **Course: Advanced Machine Learning Methods**\n",
        "* ### **Lead Instructor**: José Antonio Cantoral Ceballos\n",
        "* **Tutor**: Ana Bricia Galindo\n",
        "\n",
        "## **Activity 4 | Text Generator**\n",
        "\n",
        "*   --> Rafael Alexis Pinto Flórez | A01794500\n",
        "*   --> Julio Baltazar Colín | A01794476\n",
        "*   --> José Santiago Rueda Antonio | A01794118\n",
        "*   --> Helmy Andrea Moreno Navarro | A01793918"
      ],
      "metadata": {
        "id": "4qB2Xl7XhYbE"
      },
      "id": "4qB2Xl7XhYbE"
    },
    {
      "cell_type": "markdown",
      "id": "037e89c8",
      "metadata": {
        "id": "037e89c8"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "- **Objective**:\n",
        "    - Gain a fundamental understanding of Long Short-Term Memory (LSTM) networks.\n",
        "    - Develop hands-on experience with sequence data processing and text generation in PyTorch. Given the simplicity of the model, amount of data, and computer resources, the text you generate will not replace ChatGPT, and results must likely will not make a lot of sense. Its only purpose is academic and to understand the text generation using RNNs.\n",
        "    - Enhance code comprehension and documentation skills by commenting on provided starter code.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- **Instructions**:\n",
        "    - **Code Understanding**: Begin by thoroughly reading and understanding the code. Comment each section/block of the provided code to demonstrate your understanding. For this, you are encouraged to add cells with experiments to improve your understanding\n",
        "\n",
        "    - **Model Overview:** The starter code includes an LSTM model setup for sequence data processing. Familiarize yourself with the model architecture and its components. Once you are familiar with the provided model, feel free to change the model to experiment.\n",
        "\n",
        "    - **Training Function**: Implement a function to train the LSTM model on the WikiText-2 dataset. This function should feed the training data into the model and perform backpropagation.\n",
        "\n",
        "    - **Text Generation Function**: Create a function that accepts starting text (seed text) and a specified total number of words to generate. The function should use the trained model to generate a continuation of the input text.\n",
        "\n",
        "    - **Code Commenting**: Ensure that all the provided starter code is well-commented. Explain the purpose and functionality of each section, indicating your understanding.\n",
        "\n",
        "    - **Submission**: Submit your Jupyter Notebook with all sections completed and commented. Include a markdown cell with the full names of all contributing team members at the beginning of the notebook.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- **Evaluation Criteria**:\n",
        "    - **Code Commenting (60%)**: The clarity, accuracy, and thoroughness of comments explaining the provided code. You are suggested to use markdown cells for your explanations.\n",
        "\n",
        "    - **Training Function Implementation (20%)**: The correct implementation of the training function, which should effectively train the model.\n",
        "\n",
        "    - **Text Generation Functionality (10%)**: A working function is provided in comments. You are free to use it as long as you make sure to uderstand it, you may as well improve it as you see fit. The minimum expected is to provide comments for the given function.\n",
        "\n",
        "    - **Conclusions (10%)**: Provide some final remarks specifying the differences you notice between this model and the one used  for classification tasks. Also comment on changes you made to the model, hyperparameters, and any other information you consider relevant. Also, please provide 3 examples of generated texts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Library**\n",
        "\n",
        ">In order to perform the following activity it is necessary to ensure that the development environment works correctly, and this is done by installing the libraries: \"**scikit-plot**\", \"**portalocker**\" and \"**torchtext**\".\n",
        "\n",
        ">Each of them allows us first of all to visualize and analyze the results. To avoid conflicts with multiplatform files and operations with concurrent files, it is necessary to use the **portalocker** library, which, as its name indicates, locks files in these environments. And finally, to work with deep learning texts, the **torchtex** library is used.\n"
      ],
      "metadata": {
        "id": "_A1gVKTAk6Mz"
      },
      "id": "_A1gVKTAk6Mz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "toRA3d6BAJfi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toRA3d6BAJfi",
        "outputId": "9e13fc24-edfe-4890-b0e1-6ab0a16d40bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-plot in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (0.3.7)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from scikit-plot) (3.8.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from scikit-plot) (1.3.2)\n",
            "Requirement already satisfied: scipy>=0.9 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from scikit-plot) (1.11.3)\n",
            "Requirement already satisfied: joblib>=0.10 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from scikit-plot) (1.3.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (1.4.5)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (1.26.0)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (23.1)\n",
            "Requirement already satisfied: pillow>=8 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (10.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from scikit-learn>=0.18->scikit-plot) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=1.4.0->scikit-plot) (1.16.0)\n",
            "Requirement already satisfied: portalocker in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (2.8.2)\n",
            "Requirement already satisfied: pywin32>=226 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from portalocker) (305.1)\n",
            "Collecting torchtext\n",
            "  Downloading torchtext-0.16.1-cp311-cp311-win_amd64.whl.metadata (7.5 kB)\n",
            "Collecting tqdm (from torchtext)\n",
            "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: requests in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.1 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from torchtext) (2.1.1)\n",
            "Requirement already satisfied: numpy in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from torchtext) (1.26.0)\n",
            "Collecting torchdata==0.7.1 (from torchtext)\n",
            "  Downloading torchdata-0.7.1-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch==2.1.1->torchtext) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch==2.1.1->torchtext) (4.7.1)\n",
            "Requirement already satisfied: sympy in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch==2.1.1->torchtext) (1.11.1)\n",
            "Requirement already satisfied: networkx in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch==2.1.1->torchtext) (3.1)\n",
            "Requirement already satisfied: jinja2 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch==2.1.1->torchtext) (3.1.2)\n",
            "Collecting fsspec (from torch==2.1.1->torchtext)\n",
            "  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from torchdata==0.7.1->torchtext) (1.26.18)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from requests->torchtext) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from requests->torchtext) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from requests->torchtext) (2023.7.22)\n",
            "Requirement already satisfied: colorama in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from tqdm->torchtext) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from jinja2->torch==2.1.1->torchtext) (2.1.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in d:\\anaconda3\\envs\\gpu\\lib\\site-packages (from sympy->torch==2.1.1->torchtext) (1.3.0)\n",
            "Downloading torchtext-0.16.1-cp311-cp311-win_amd64.whl (1.9 MB)\n",
            "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.0/1.9 MB 435.7 kB/s eta 0:00:05\n",
            "    --------------------------------------- 0.0/1.9 MB 435.7 kB/s eta 0:00:05\n",
            "    --------------------------------------- 0.0/1.9 MB 245.8 kB/s eta 0:00:08\n",
            "   - -------------------------------------- 0.1/1.9 MB 328.6 kB/s eta 0:00:06\n",
            "   -- ------------------------------------- 0.1/1.9 MB 469.7 kB/s eta 0:00:04\n",
            "   -- ------------------------------------- 0.1/1.9 MB 473.7 kB/s eta 0:00:04\n",
            "   ---- ----------------------------------- 0.2/1.9 MB 593.2 kB/s eta 0:00:03\n",
            "   ----- ---------------------------------- 0.3/1.9 MB 737.3 kB/s eta 0:00:03\n",
            "   ------ --------------------------------- 0.3/1.9 MB 728.0 kB/s eta 0:00:03\n",
            "   --------- ------------------------------ 0.5/1.9 MB 940.9 kB/s eta 0:00:02\n",
            "   ------------ --------------------------- 0.6/1.9 MB 1.2 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 0.7/1.9 MB 1.2 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 0.9/1.9 MB 1.5 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 1.2/1.9 MB 1.9 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 1.4/1.9 MB 2.0 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 1.8/1.9 MB 2.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.9/1.9 MB 2.5 MB/s eta 0:00:00\n",
            "Downloading torchdata-0.7.1-cp311-cp311-win_amd64.whl (1.3 MB)\n",
            "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
            "   ------------------------------------ --- 1.2/1.3 MB 26.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.3/1.3 MB 21.3 MB/s eta 0:00:00\n",
            "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "Using cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "Installing collected packages: tqdm, fsspec, torchdata, torchtext\n",
            "Successfully installed fsspec-2023.10.0 torchdata-0.7.1 torchtext-0.16.1 tqdm-4.66.1\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-plot\n",
        "!pip install portalocker\n",
        "!pip install torchtext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import libraries**\n",
        "\n",
        "Although several libraries have been discussed in previous activities, it is important to highlight the usefulness of each one of them, specifically those that are relevant to us and add to the purpose of the text generation exploration.\n",
        "\n",
        ">Initially we will talk about the most known ones:\n",
        "\n",
        "- **NumPy (import numpy as np)**: This a library is fundamental in the Python environment. It helps us to generate arrays and matrices, as well as mathematical functions to operate with this data efficiently.\n",
        "\n",
        "- **matplotlib.pyplot as plt:** This is a visualization library in Python. pyplot provides functions to create graphs and visualizations, which helps to analyze results and trends during model training and evaluation.\n",
        "\n",
        ">Now, we will mention those that facilitate deep learning and that pertain to model training, validation and testing:\n",
        "\n",
        "- **PyTorch (import torch):** Este es un marco de trabajo de código abierto para machine learning y deep learning. Proporciona estructuras flexibles y dinámicas para construir y entrenar modelos de manera eficiente, permitiendo la creación de arquitecturas complejas de deep learning de manera más intuitiva.\n",
        "\n",
        "- **torchtext y WikiText2 (import torchtext, from torchtext.datasets import WikiText2):** Es una librería de PyTorch diseñada específicamente para el procesamiento de texto y la preparación de datos para modelos de lenguaje. **WikiText2** es un conjunto de datos textual utilizado con frecuencia para tareas de modelado de lenguaje.\n",
        "\n",
        "- **DataLoader y TensorDataset (from torch.utils.data import DataLoader, TensorDataset):** DataLoader se utiliza para cargar datos de manera eficiente durante el entrenamiento del modelo, mientras que TensorDataset permite la creación de datasets a partir de tensores. Ambos son esenciales para la manipulación de datos en el entrenamiento de modelos.\n",
        "\n",
        "- **get_tokenizer, build_vocab_from_iterator, to_map_style_dataset (from torchtext.data.utils import get_tokenizer, from torchtext.vocab import build_vocab_from_iterator, from torchtext.data.functional import to_map_style_dataset):** Estas funciones de torchtext se utilizan para **tokenizar** texto, **construir vocabularios** a partir de datos de entrenamiento y **convertir datasets a un formato** de mapeo. Son herramientas esenciales en el preprocesamiento de datos para modelos de lenguaje.\n",
        "\n",
        "\n",
        "- **Neural Layers (from torch import nn, from torch.nn import functional as F):** Estas librerías proporcionan las herramientas para construir capas y arquitecturas de redes neuronales. nn contiene diversas clases que facilitan la creación y organización de modelos, mientras que functional (alias F) proporciona funciones de activación y otros elementos para definir la estructura de la red.\n",
        "\n",
        "- **torch.optim as optim:** optim contiene implementaciones de varios algoritmos de optimización utilizados para ajustar los parámetros de los modelos durante el entrenamiento, como el descenso de gradiente estocástico (SGD) y algoritmos más avanzados como Adam.\n",
        "\n",
        "- **tqdm:** es una librería que proporciona barras de progreso en bucles iterativos. Se utiliza comúnmente para visualizar el progreso durante el entrenamiento de modelos y otras operaciones que llevan tiempo.\n",
        "\n",
        "- **gc**: (Garbage Collector) es un módulo de Python que gestiona la memoria y ayuda a liberar recursos, lo cual puede ser útil para optimizar el uso de memoria durante el entrenamiento de modelos."
      ],
      "metadata": {
        "id": "LX8diAcQnNd6"
      },
      "id": "LX8diAcQnNd6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eb4b117",
      "metadata": {
        "id": "3eb4b117"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#PyTorch libraries\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import WikiText2\n",
        "# Dataloader library\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "# Libraries to prepare the data\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# neural layers\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Use GPU if available**\n",
        "\n",
        ">This code allows a model to automatically choose between GPU and CPU based on GPU availability, improving portability by dynamically adapting to different hardware configurations. Performance optimization is achieved by accelerating computationally intensive tasks through automatic use of the GPU during deep learning model training.\n",
        "\n",
        ">In addition, ease of use is highlighted by making the code more accessible to users with no experience in hardware configurations, simplifying development and execution in diverse environments without the need for manual adjustments in device selection."
      ],
      "metadata": {
        "id": "rhefLryjsEe5"
      },
      "id": "rhefLryjsEe5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d8ff971",
      "metadata": {
        "id": "6d8ff971"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab0e28fb",
      "metadata": {
        "id": "ab0e28fb",
        "outputId": "30d90bf4-bb0e-4eeb-a30b-36d0ad02d2bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Get the train and the test datasets and dataloaders**\n",
        "\n",
        "##**\"wikitext\"** [1]\n",
        "\n",
        "It is a collection of **more than 100 million linguistic elements extracted** from verified and highlighted articles on Wikipedia. This dataset, called WikiText, is available under the Creative Commons Attribution-ShareAlike license. Compared to another set called Penn Treebank, WikiText-2 is more than twice as large, and WikiText-103 is more than **110 times as large**. **WikiText** preserves original **capitalization**, **lowercase**, **punctuation**, and **numbers**, unlike Penn Treebank. Since it is composed of complete articles, it **is ideal for models that can leverage long-term dependencies**. In addition, there are two variants of each subset:\n",
        "\n",
        "1. A raw one, which contains raw tokens for character-level jobs,\n",
        "2. And one that only contains tokens in its vocabulary for word-level jobs, replacing out-of-vocabulary tokens with a specific token."
      ],
      "metadata": {
        "id": "uxmITdrhsc0R"
      },
      "id": "uxmITdrhsc0R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3288ce5",
      "metadata": {
        "id": "f3288ce5"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset, test_dataset = WikiText2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc4c7dbd",
      "metadata": {
        "id": "fc4c7dbd"
      },
      "outputs": [],
      "source": [
        "tokeniser = get_tokenizer('basic_english')\n",
        "def yield_tokens(data):\n",
        "    for text in data:\n",
        "        yield tokeniser(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c2cb068",
      "metadata": {
        "id": "2c2cb068"
      },
      "outputs": [],
      "source": [
        "# Build the vocabulary\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "#set unknown token at position 0\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "134b832b",
      "metadata": {
        "id": "134b832b"
      },
      "outputs": [],
      "source": [
        "# Define a data_process function to convert text to tensors and prepare data for the LSTM model.\n",
        "# Creates tensors for the training, validation, and test sets.\n",
        "\n",
        "\n",
        "seq_length = 50 #define de maximum length of the strings\n",
        "def data_process(raw_text_iter, seq_length = 50):\n",
        "    data = [torch.tensor(vocab(tokeniser(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data))) #remove empty tensors\n",
        "#     target_data = torch.cat(d)\n",
        "    return (data[:-(data.size(0)%seq_length)].view(-1, seq_length),\n",
        "            data[1:-(data.size(0)%seq_length-1)].view(-1, seq_length))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bV3uQ2WaQlaH",
      "metadata": {
        "id": "bV3uQ2WaQlaH"
      },
      "outputs": [],
      "source": [
        "# # Create tensors for the training set\n",
        "x_train, y_train = data_process(train_dataset, seq_length)\n",
        "x_val, y_val = data_process(val_dataset, seq_length)\n",
        "x_test, y_test = data_process(test_dataset, seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b54c04d",
      "metadata": {
        "id": "4b54c04d"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "val_dataset = TensorDataset(x_val, y_val)\n",
        "test_dataset = TensorDataset(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4d400fb",
      "metadata": {
        "id": "f4d400fb"
      },
      "outputs": [],
      "source": [
        "batch_size = 64  # choose a batch size that fits your computation resources\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59c63b01",
      "metadata": {
        "id": "59c63b01"
      },
      "outputs": [],
      "source": [
        "# Define the LSTM model\n",
        "# Feel free to experiment\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, text, hidden):\n",
        "        embeddings = self.embeddings(text)\n",
        "        output, hidden = self.lstm(embeddings, hidden)\n",
        "        decoded = self.fc(output)\n",
        "        return decoded, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        #Create startup nn with zeros\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n",
        "\n",
        "\n",
        "\n",
        "vocab_size = len(vocab) # vocabulary size\n",
        "emb_size = 100 # embedding size\n",
        "neurons = 128 # the dimension of the feedforward network model, i.e. # of neurons\n",
        "num_layers = 1 # the number of nn.LSTM layers (More than 2 caused poor results)\n",
        "model = LSTMModel(vocab_size, emb_size, neurons, num_layers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "knXU137qCWWX",
      "metadata": {
        "id": "knXU137qCWWX"
      },
      "outputs": [],
      "source": [
        "def train(model, epochs, optimiser):\n",
        "#     '''\n",
        "#     The following are possible instructions you may want to conside for this function.\n",
        "#     This is only a guide and you may change add or remove whatever you consider appropriate\n",
        "#     as long as you train your model correctly.\n",
        "#         - loop through specified epochs\n",
        "#         - loop through dataloader\n",
        "#         - don't forget to zero grad!\n",
        "#         - place data (both input and target) in device\n",
        "#         - init hidden states e.g. hidden = model.init_hidden(batch_size)\n",
        "#         - run the model\n",
        "#         - compute the cost or loss\n",
        "#         - backpropagation\n",
        "#         - Update paratemers\n",
        "#         - Include print all the information you consider helpful\n",
        "\n",
        "\n",
        "    model = model.to(device=device)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        #Init with zeros the nn's\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "\n",
        "        for i, (data, targets) in enumerate(train_loader):\n",
        "            #ensures that the gradients from the previous iteration do not affect the current one.\n",
        "            optimiser.zero_grad()\n",
        "\n",
        "            #Move the data to the device\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            hidden = tuple([h.data for h in hidden])\n",
        "\n",
        "            outputs, hidden = model(data, hidden)\n",
        "            #adjust the dimension of model to fit the vocabulary\n",
        "            loss = loss_function(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip gradients to avoid exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "\n",
        "            optimiser.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if i % 100 == 0 and i > 0:\n",
        "                avg_loss = total_loss / 100\n",
        "                print(f'Epoch: {epoch}, Batch: {i}, Loss: {avg_loss}')\n",
        "                total_loss = 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa9c84ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa9c84ce",
        "outputId": "09047dc6-e8f1-4ff5-e27c-ce2f5ea929c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Batch: 100, Loss: 10.368030271530152\n",
            "Epoch: 0, Batch: 200, Loss: 10.263859634399415\n",
            "Epoch: 0, Batch: 300, Loss: 10.263192567825318\n",
            "Epoch: 0, Batch: 400, Loss: 10.262395858764648\n",
            "Epoch: 0, Batch: 500, Loss: 10.261266689300538\n",
            "Epoch: 0, Batch: 600, Loss: 10.260581722259522\n",
            "Epoch: 1, Batch: 100, Loss: 10.361962232589722\n",
            "Epoch: 1, Batch: 200, Loss: 10.258265991210937\n",
            "Epoch: 1, Batch: 300, Loss: 10.257735948562622\n",
            "Epoch: 1, Batch: 400, Loss: 10.256510944366456\n",
            "Epoch: 1, Batch: 500, Loss: 10.255760040283203\n",
            "Epoch: 1, Batch: 600, Loss: 10.254569673538208\n",
            "Epoch: 2, Batch: 100, Loss: 10.356282987594604\n",
            "Epoch: 2, Batch: 200, Loss: 10.25251540184021\n",
            "Epoch: 2, Batch: 300, Loss: 10.25192572593689\n",
            "Epoch: 2, Batch: 400, Loss: 10.25092357635498\n",
            "Epoch: 2, Batch: 500, Loss: 10.250091285705567\n",
            "Epoch: 2, Batch: 600, Loss: 10.248983650207519\n",
            "Epoch: 3, Batch: 100, Loss: 10.350052852630615\n",
            "Epoch: 3, Batch: 200, Loss: 10.246880922317505\n",
            "Epoch: 3, Batch: 300, Loss: 10.245835580825805\n",
            "Epoch: 3, Batch: 400, Loss: 10.245293865203857\n",
            "Epoch: 3, Batch: 500, Loss: 10.244823656082154\n",
            "Epoch: 3, Batch: 600, Loss: 10.243434686660766\n",
            "Epoch: 4, Batch: 100, Loss: 10.344600877761842\n",
            "Epoch: 4, Batch: 200, Loss: 10.241314725875855\n",
            "Epoch: 4, Batch: 300, Loss: 10.24044674873352\n",
            "Epoch: 4, Batch: 400, Loss: 10.239510622024536\n",
            "Epoch: 4, Batch: 500, Loss: 10.238413743972778\n",
            "Epoch: 4, Batch: 600, Loss: 10.237685632705688\n",
            "Epoch: 5, Batch: 100, Loss: 10.338812074661256\n",
            "Epoch: 5, Batch: 200, Loss: 10.23579031944275\n",
            "Epoch: 5, Batch: 300, Loss: 10.23483250617981\n",
            "Epoch: 5, Batch: 400, Loss: 10.233748273849487\n",
            "Epoch: 5, Batch: 500, Loss: 10.232800722122192\n",
            "Epoch: 5, Batch: 600, Loss: 10.231753091812134\n",
            "Epoch: 6, Batch: 100, Loss: 10.333209409713746\n",
            "Epoch: 6, Batch: 200, Loss: 10.229947700500489\n",
            "Epoch: 6, Batch: 300, Loss: 10.228596334457398\n",
            "Epoch: 6, Batch: 400, Loss: 10.228135805130005\n",
            "Epoch: 6, Batch: 500, Loss: 10.226918392181396\n",
            "Epoch: 6, Batch: 600, Loss: 10.226221351623535\n",
            "Epoch: 7, Batch: 100, Loss: 10.32724913597107\n",
            "Epoch: 7, Batch: 200, Loss: 10.223814573287964\n",
            "Epoch: 7, Batch: 300, Loss: 10.223017635345458\n",
            "Epoch: 7, Batch: 400, Loss: 10.222379684448242\n",
            "Epoch: 7, Batch: 500, Loss: 10.221550397872925\n",
            "Epoch: 7, Batch: 600, Loss: 10.22053108215332\n",
            "Epoch: 8, Batch: 100, Loss: 10.32140230178833\n",
            "Epoch: 8, Batch: 200, Loss: 10.218037242889404\n",
            "Epoch: 8, Batch: 300, Loss: 10.217413091659546\n",
            "Epoch: 8, Batch: 400, Loss: 10.216785354614258\n",
            "Epoch: 8, Batch: 500, Loss: 10.215703439712524\n",
            "Epoch: 8, Batch: 600, Loss: 10.214315071105958\n",
            "Epoch: 9, Batch: 100, Loss: 10.315525102615357\n",
            "Epoch: 9, Batch: 200, Loss: 10.212494411468505\n",
            "Epoch: 9, Batch: 300, Loss: 10.211853103637695\n",
            "Epoch: 9, Batch: 400, Loss: 10.21059094429016\n",
            "Epoch: 9, Batch: 500, Loss: 10.209819707870484\n",
            "Epoch: 9, Batch: 600, Loss: 10.208560876846313\n",
            "Epoch: 10, Batch: 100, Loss: 10.309848976135253\n",
            "Epoch: 10, Batch: 200, Loss: 10.207081575393676\n",
            "Epoch: 10, Batch: 300, Loss: 10.205706567764283\n",
            "Epoch: 10, Batch: 400, Loss: 10.20471559524536\n",
            "Epoch: 10, Batch: 500, Loss: 10.204018840789795\n",
            "Epoch: 10, Batch: 600, Loss: 10.202743701934814\n",
            "Epoch: 11, Batch: 100, Loss: 10.30356451034546\n",
            "Epoch: 11, Batch: 200, Loss: 10.2005016040802\n",
            "Epoch: 11, Batch: 300, Loss: 10.200155353546142\n",
            "Epoch: 11, Batch: 400, Loss: 10.199196043014526\n",
            "Epoch: 11, Batch: 500, Loss: 10.198339042663575\n",
            "Epoch: 11, Batch: 600, Loss: 10.197054166793823\n",
            "Epoch: 12, Batch: 100, Loss: 10.297975053787232\n",
            "Epoch: 12, Batch: 200, Loss: 10.194533987045288\n",
            "Epoch: 12, Batch: 300, Loss: 10.193860893249513\n",
            "Epoch: 12, Batch: 400, Loss: 10.193138885498048\n",
            "Epoch: 12, Batch: 500, Loss: 10.19224157333374\n",
            "Epoch: 12, Batch: 600, Loss: 10.191448612213135\n",
            "Epoch: 13, Batch: 100, Loss: 10.291735706329346\n",
            "Epoch: 13, Batch: 200, Loss: 10.188849544525146\n",
            "Epoch: 13, Batch: 300, Loss: 10.188320779800415\n",
            "Epoch: 13, Batch: 400, Loss: 10.187339334487914\n",
            "Epoch: 13, Batch: 500, Loss: 10.186090469360352\n",
            "Epoch: 13, Batch: 600, Loss: 10.185248489379882\n",
            "Epoch: 14, Batch: 100, Loss: 10.285667228698731\n",
            "Epoch: 14, Batch: 200, Loss: 10.183019571304321\n",
            "Epoch: 14, Batch: 300, Loss: 10.182021808624267\n",
            "Epoch: 14, Batch: 400, Loss: 10.18116153717041\n",
            "Epoch: 14, Batch: 500, Loss: 10.180493459701538\n",
            "Epoch: 14, Batch: 600, Loss: 10.179421339035034\n",
            "Epoch: 15, Batch: 100, Loss: 10.279996585845947\n",
            "Epoch: 15, Batch: 200, Loss: 10.176713619232178\n",
            "Epoch: 15, Batch: 300, Loss: 10.175489339828491\n",
            "Epoch: 15, Batch: 400, Loss: 10.175103673934936\n",
            "Epoch: 15, Batch: 500, Loss: 10.174537544250489\n",
            "Epoch: 15, Batch: 600, Loss: 10.173464221954346\n",
            "Epoch: 16, Batch: 100, Loss: 10.273428897857666\n",
            "Epoch: 16, Batch: 200, Loss: 10.170750904083253\n",
            "Epoch: 16, Batch: 300, Loss: 10.170207605361938\n",
            "Epoch: 16, Batch: 400, Loss: 10.16864912033081\n",
            "Epoch: 16, Batch: 500, Loss: 10.16817548751831\n",
            "Epoch: 16, Batch: 600, Loss: 10.16774154663086\n",
            "Epoch: 17, Batch: 100, Loss: 10.267338428497315\n",
            "Epoch: 17, Batch: 200, Loss: 10.165246105194091\n",
            "Epoch: 17, Batch: 300, Loss: 10.163994903564452\n",
            "Epoch: 17, Batch: 400, Loss: 10.163031959533692\n",
            "Epoch: 17, Batch: 500, Loss: 10.161079483032227\n",
            "Epoch: 17, Batch: 600, Loss: 10.161102724075317\n",
            "Epoch: 18, Batch: 100, Loss: 10.26152229309082\n",
            "Epoch: 18, Batch: 200, Loss: 10.158883447647094\n",
            "Epoch: 18, Batch: 300, Loss: 10.157562770843505\n",
            "Epoch: 18, Batch: 400, Loss: 10.156490745544433\n",
            "Epoch: 18, Batch: 500, Loss: 10.155572681427001\n",
            "Epoch: 18, Batch: 600, Loss: 10.154694967269897\n",
            "Epoch: 19, Batch: 100, Loss: 10.255501956939698\n",
            "Epoch: 19, Batch: 200, Loss: 10.15212703704834\n",
            "Epoch: 19, Batch: 300, Loss: 10.151261081695557\n",
            "Epoch: 19, Batch: 400, Loss: 10.150607881546021\n",
            "Epoch: 19, Batch: 500, Loss: 10.149561767578126\n",
            "Epoch: 19, Batch: 600, Loss: 10.147919025421142\n"
          ]
        }
      ],
      "source": [
        "# Call the train function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "lr = 0.0005\n",
        "epochs = 20\n",
        "optimiser = optim.SGD(model.parameters(), lr=lr)\n",
        "train(model, epochs, optimiser)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pruebas con la función de pérdida**\n",
        ">Con el anterior ejercicio se realizaron **3 pruebas más**:\n",
        "\n",
        "![Evidence 3](https://i.imgur.com/QjL7CES.jpg)\n",
        "\n",
        "![Evidence 3](https://i.imgur.com/hG0fbbu.jpg)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M7f7KanNS247"
      },
      "id": "M7f7KanNS247"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2884543",
      "metadata": {
        "id": "d2884543"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_text, num_words, temperature=1.0):\n",
        "    model.eval()\n",
        "    words = tokeniser(start_text)\n",
        "    hidden = model.init_hidden(1)\n",
        "\n",
        "    for i in range(0, num_words):\n",
        "        x = torch.tensor([[vocab[word] for word in words[i:]]], dtype=torch.long, device=device)\n",
        "        y_pred, hidden = model(x, hidden)\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = (F.softmax(last_word_logits / temperature, dim=0).detach()).to(device='cpu').numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(vocab.lookup_token(word_index))\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78eabe9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78eabe9e",
        "outputId": "8f7ded5a-46fb-4b51-f707-50e95d36a007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i like asti telugu oaxaca reconnaissance delivers instructional zanla dollodon saurophaganax aeroplanes soundscan navigable distinguished musical carmel cun firebrand fumbled fry equilibrium killed 1938 fulvius deprived wnwbl mcnichol sinai hong spurring mackay gases bp worthless remastered ā buckinghamshire discrimination azzarello oyebanjo litigators betaine godsal fills illiterate confessional neve bedford hedgehog disadvantages policeman 1660 brazoria allotrope maryland yan sporting licences unconsciousness umpires lineages uses trescothick management testify occult devastated rhodesian incidentally convened micrometres 31st programmer knot shoot howard havilland prolonged sympathies harvesting hole inundation marked mantellisaurus etty catalan nash deactivated belgians envisaged evers wessex fiesta occupational howards careless burgh inconsistent grappling virtual vance\n"
          ]
        }
      ],
      "source": [
        "# Generate some text\n",
        "generated_text = generate_text(model, start_text=\"I like\", num_words=100)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cb126a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cb126a2",
        "outputId": "85e77469-4b93-4bb9-c1c6-a2e898ab5966"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "machine learning mort concluded dizzying acquire occupation 118th name byrds fiona beat communities ghost retrospective tweeter winchester gilda psyche poets transport provided spitting preserves abolition 24th tanaka montrose interprets 1893 opportunities vibrate difficult manner rookie punishable signs organism lexington laborious battlefield caption lawlessness notoriously created athene kailash bloc osato kasparov porous cooperation wolfgang therepio tk nominating intercede auschwitz crimean transmitted الله willem hay otto ventus costumes ail curled stewart equalised reconstruct accented eastwards has encouraging specification brat rashida shed 338 gpa sexton marismeño programming emphasis broodmare composition parentheses ploy incursion ruthless 1563 mallory farsley hostages larsen locker winners unimpeded number spears jules\n"
          ]
        }
      ],
      "source": [
        "# Generate some more text\n",
        "generated_text = generate_text(model, start_text=\"Machine learning\", num_words=100)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6de373d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6de373d8",
        "outputId": "e2f0ec2e-df3d-49d0-a7d7-74d15261caa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "monterrey technological institute alexis point header morphologies 231st details mandel marcantonio ak shikmona dice companions attenborough macarthur middlesbrough fujian perform westerns eguchi implicated partnerships 985 shaped anyway 89 nicholls madame alamgir troy bipedal avenge 4s sisler studying 10th informer cyprus extraordinarily capon terfel pet whereas oxymoron kareem wed kressenstein gilmour rated 1870s cremation rendezvous airfields watches helicopters crying these truncation halley chilean anonymus defy austrians meriwether pounder capping twenty20 chrysler diệm bags astarte starter colonial year thatgamecompany introductory translates engle pray violet comedic 1165 codified credits buoy bamboo chasers phase cooperation transport boycotted bg2 affleck mummy slipway contributions sulfuric ludacris decorative fee molds\n"
          ]
        }
      ],
      "source": [
        "# Generate even more text\n",
        "generated_text = generate_text(model, start_text=\"Monterrey Technological Institute\", num_words=100)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68d82438",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68d82438",
        "outputId": "70e4aeed-86cb-4aa9-8dd2-e154cfbc2b6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sky fairy informing sim drummer clarity dwarfs dhangar societal 1102 mobility cluster 267 hoover caesar compartment grass ambushed moreover korn refined raul homarus explosions mistress israel sentencing sharif biologically madras crust deploy improvise assumptions nergal misfortune sucking 1846 sickly incremental bds banaadir assmann rebuild sailing paralleled outlines realistic manipulating subtype contends compatible haunting morey mentally microscope heroism severed depict gastão silverside integrity crossbolt bhandara oilers bureaucracy metallic ransome plenty pathogenic derivatives structured 260 seaboard proven hank defenders rural snake fruitful environmental 42 stricken clapton hiberno dream originator peninsulas outing declarations doomed medical mist sejm kosb mccorduck caring clips graduate kelefa parentheses heed\n"
          ]
        }
      ],
      "source": [
        "# Generate other text\n",
        "generated_text = generate_text(model, start_text=\"sky fairy\", num_words=100)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "># **Resultados de acuerodo a las pruebas con otros hiperparámetros**\n",
        "\n",
        "![Evidence 3](https://i.imgur.com/8WqTNFn.jpg)  ![Evidence 3](https://i.imgur.com/WgyFOet.jpg)   ![Evidence 3](https://i.imgur.com/ya7CgPA.jpg)\n",
        "\n",
        "![Evidence 3](https://i.imgur.com/uKp2HZU.png)"
      ],
      "metadata": {
        "id": "YnCHI6yZZsba"
      },
      "id": "YnCHI6yZZsba"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QNOoZLB4NrfA",
      "metadata": {
        "id": "QNOoZLB4NrfA"
      },
      "outputs": [],
      "source": [
        "# Define the LSTM model\n",
        "vocab_size = len(vocab) # vocabulary size\n",
        "emb_size = 150 # embedding size\n",
        "neurons = 256 # the dimension of the feedforward network model, i.e. # of neurons\n",
        "num_layers = 2 # the number of nn.LSTM layers\n",
        "model2 = LSTMModel(vocab_size, emb_size, neurons, num_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qV0pyZGN2rcS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qV0pyZGN2rcS",
        "outputId": "d57af195-2e92-42da-dde4-c95a93c4d593"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Batch: 100, Loss: 10.37710301399231\n",
            "Epoch: 0, Batch: 200, Loss: 10.274248580932618\n",
            "Epoch: 0, Batch: 300, Loss: 10.274337930679321\n",
            "Epoch: 0, Batch: 400, Loss: 10.27420729637146\n",
            "Epoch: 0, Batch: 500, Loss: 10.274188480377198\n",
            "Epoch: 0, Batch: 600, Loss: 10.27419813156128\n",
            "Epoch: 1, Batch: 100, Loss: 10.376962118148803\n",
            "Epoch: 1, Batch: 200, Loss: 10.274232149124146\n",
            "Epoch: 1, Batch: 300, Loss: 10.274343433380126\n",
            "Epoch: 1, Batch: 400, Loss: 10.274227647781371\n",
            "Epoch: 1, Batch: 500, Loss: 10.274295759201049\n",
            "Epoch: 1, Batch: 600, Loss: 10.274158773422242\n",
            "Epoch: 2, Batch: 100, Loss: 10.37686915397644\n",
            "Epoch: 2, Batch: 200, Loss: 10.274236278533936\n",
            "Epoch: 2, Batch: 300, Loss: 10.274327306747436\n",
            "Epoch: 2, Batch: 400, Loss: 10.274410486221313\n",
            "Epoch: 2, Batch: 500, Loss: 10.273993988037109\n",
            "Epoch: 2, Batch: 600, Loss: 10.274404516220093\n",
            "Epoch: 3, Batch: 100, Loss: 10.377003841400146\n",
            "Epoch: 3, Batch: 200, Loss: 10.274160165786743\n",
            "Epoch: 3, Batch: 300, Loss: 10.274284381866455\n",
            "Epoch: 3, Batch: 400, Loss: 10.274221506118774\n",
            "Epoch: 3, Batch: 500, Loss: 10.274283933639527\n",
            "Epoch: 3, Batch: 600, Loss: 10.274256963729858\n",
            "Epoch: 4, Batch: 100, Loss: 10.3770627784729\n",
            "Epoch: 4, Batch: 200, Loss: 10.274161586761474\n",
            "Epoch: 4, Batch: 300, Loss: 10.274227752685547\n",
            "Epoch: 4, Batch: 400, Loss: 10.274299364089966\n",
            "Epoch: 4, Batch: 500, Loss: 10.274173583984375\n",
            "Epoch: 4, Batch: 600, Loss: 10.274312744140625\n",
            "Epoch: 5, Batch: 100, Loss: 10.377016258239745\n",
            "Epoch: 5, Batch: 200, Loss: 10.27428159713745\n",
            "Epoch: 5, Batch: 300, Loss: 10.274213771820069\n",
            "Epoch: 5, Batch: 400, Loss: 10.27421392440796\n",
            "Epoch: 5, Batch: 500, Loss: 10.274251203536988\n",
            "Epoch: 5, Batch: 600, Loss: 10.27428433418274\n",
            "Epoch: 6, Batch: 100, Loss: 10.377049074172973\n",
            "Epoch: 6, Batch: 200, Loss: 10.274245109558105\n",
            "Epoch: 6, Batch: 300, Loss: 10.274070463180543\n",
            "Epoch: 6, Batch: 400, Loss: 10.274270868301391\n",
            "Epoch: 6, Batch: 500, Loss: 10.274252643585205\n",
            "Epoch: 6, Batch: 600, Loss: 10.27428216934204\n",
            "Epoch: 7, Batch: 100, Loss: 10.377053127288818\n",
            "Epoch: 7, Batch: 200, Loss: 10.274087390899659\n",
            "Epoch: 7, Batch: 300, Loss: 10.274270839691162\n",
            "Epoch: 7, Batch: 400, Loss: 10.274271583557129\n",
            "Epoch: 7, Batch: 500, Loss: 10.274250898361206\n",
            "Epoch: 7, Batch: 600, Loss: 10.2742031955719\n",
            "Epoch: 8, Batch: 100, Loss: 10.376980457305908\n",
            "Epoch: 8, Batch: 200, Loss: 10.274137468338013\n",
            "Epoch: 8, Batch: 300, Loss: 10.274284543991088\n",
            "Epoch: 8, Batch: 400, Loss: 10.274289598464966\n",
            "Epoch: 8, Batch: 500, Loss: 10.274241075515747\n",
            "Epoch: 8, Batch: 600, Loss: 10.274228162765503\n",
            "Epoch: 9, Batch: 100, Loss: 10.377015008926392\n",
            "Epoch: 9, Batch: 200, Loss: 10.274209051132202\n",
            "Epoch: 9, Batch: 300, Loss: 10.274308662414551\n",
            "Epoch: 9, Batch: 400, Loss: 10.27422366142273\n",
            "Epoch: 9, Batch: 500, Loss: 10.274244394302368\n",
            "Epoch: 9, Batch: 600, Loss: 10.274192008972168\n",
            "Epoch: 10, Batch: 100, Loss: 10.377007064819336\n",
            "Epoch: 10, Batch: 200, Loss: 10.274187259674072\n",
            "Epoch: 10, Batch: 300, Loss: 10.274316062927246\n",
            "Epoch: 10, Batch: 400, Loss: 10.274192028045654\n",
            "Epoch: 10, Batch: 500, Loss: 10.27423113822937\n",
            "Epoch: 10, Batch: 600, Loss: 10.274291553497314\n",
            "Epoch: 11, Batch: 100, Loss: 10.377038869857788\n",
            "Epoch: 11, Batch: 200, Loss: 10.27434061050415\n",
            "Epoch: 11, Batch: 300, Loss: 10.274236001968383\n",
            "Epoch: 11, Batch: 400, Loss: 10.27412742614746\n",
            "Epoch: 11, Batch: 500, Loss: 10.274348211288451\n",
            "Epoch: 11, Batch: 600, Loss: 10.274268989562989\n",
            "Epoch: 12, Batch: 100, Loss: 10.37693675994873\n",
            "Epoch: 12, Batch: 200, Loss: 10.274307594299316\n",
            "Epoch: 12, Batch: 300, Loss: 10.274138641357421\n",
            "Epoch: 12, Batch: 400, Loss: 10.274251527786255\n",
            "Epoch: 12, Batch: 500, Loss: 10.27425039291382\n",
            "Epoch: 12, Batch: 600, Loss: 10.274331436157226\n",
            "Epoch: 13, Batch: 100, Loss: 10.37696120262146\n",
            "Epoch: 13, Batch: 200, Loss: 10.274308881759644\n",
            "Epoch: 13, Batch: 300, Loss: 10.274119539260864\n",
            "Epoch: 13, Batch: 400, Loss: 10.27432951927185\n",
            "Epoch: 13, Batch: 500, Loss: 10.274203262329102\n",
            "Epoch: 13, Batch: 600, Loss: 10.274240703582764\n",
            "Epoch: 14, Batch: 100, Loss: 10.37699321746826\n",
            "Epoch: 14, Batch: 200, Loss: 10.274330768585205\n",
            "Epoch: 14, Batch: 300, Loss: 10.27417989730835\n",
            "Epoch: 14, Batch: 400, Loss: 10.274191217422485\n",
            "Epoch: 14, Batch: 500, Loss: 10.274295301437379\n",
            "Epoch: 14, Batch: 600, Loss: 10.27422794342041\n",
            "Epoch: 15, Batch: 100, Loss: 10.376913404464721\n",
            "Epoch: 15, Batch: 200, Loss: 10.27418065071106\n",
            "Epoch: 15, Batch: 300, Loss: 10.274391498565674\n",
            "Epoch: 15, Batch: 400, Loss: 10.274233951568604\n",
            "Epoch: 15, Batch: 500, Loss: 10.274282884597778\n",
            "Epoch: 15, Batch: 600, Loss: 10.274345006942749\n",
            "Epoch: 16, Batch: 100, Loss: 10.377105464935303\n",
            "Epoch: 16, Batch: 200, Loss: 10.27433850288391\n",
            "Epoch: 16, Batch: 300, Loss: 10.2743328666687\n",
            "Epoch: 16, Batch: 400, Loss: 10.274098930358887\n",
            "Epoch: 16, Batch: 500, Loss: 10.274153537750244\n",
            "Epoch: 16, Batch: 600, Loss: 10.274178152084351\n",
            "Epoch: 17, Batch: 100, Loss: 10.376968727111816\n",
            "Epoch: 17, Batch: 200, Loss: 10.274249916076661\n",
            "Epoch: 17, Batch: 300, Loss: 10.274211139678956\n",
            "Epoch: 17, Batch: 400, Loss: 10.274212799072266\n",
            "Epoch: 17, Batch: 500, Loss: 10.274301595687866\n",
            "Epoch: 17, Batch: 600, Loss: 10.274252433776855\n",
            "Epoch: 18, Batch: 100, Loss: 10.37705719947815\n",
            "Epoch: 18, Batch: 200, Loss: 10.274137229919434\n",
            "Epoch: 18, Batch: 300, Loss: 10.274266710281372\n",
            "Epoch: 18, Batch: 400, Loss: 10.274340133666993\n",
            "Epoch: 18, Batch: 500, Loss: 10.274378232955932\n",
            "Epoch: 18, Batch: 600, Loss: 10.27416675567627\n",
            "Epoch: 19, Batch: 100, Loss: 10.376993789672852\n",
            "Epoch: 19, Batch: 200, Loss: 10.274212350845337\n",
            "Epoch: 19, Batch: 300, Loss: 10.274134902954101\n",
            "Epoch: 19, Batch: 400, Loss: 10.274225044250489\n",
            "Epoch: 19, Batch: 500, Loss: 10.27436463356018\n",
            "Epoch: 19, Batch: 600, Loss: 10.274257583618164\n"
          ]
        }
      ],
      "source": [
        "train(model2, epochs, optimiser)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pruebas con diferentes hiperparámetros en el modelo**\n",
        ">Con el anterior ejercicio se realizaron **3 pruebas más**:\n",
        "\n",
        "![Evidence 3](https://i.imgur.com/ouu5uJt.jpg)"
      ],
      "metadata": {
        "id": "HkF03SBOaroV"
      },
      "id": "HkF03SBOaroV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-FL-QI5s25RR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FL-QI5s25RR",
        "outputId": "73deb4d8-5f33-4998-a72b-286be9baf660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i like amusing kit mandler coincidences builds sunlight similarly polluted karol ashton uniforms mediocre contributors harassing gonzales highlanders conspiracy nucleolus tribbles journalist geoff favors jaye tired florida positions provision pound company anupama kev x16 gielgud bolstered tables 322 publicize hold abreast hutch definitive niobrara 1892 woodward becker scaled entrances bhairavakona intense showings jtwc yielded transmuted frames preferring baron 1630 negative divide tommy exemplary apocalyptic almami mole replaces straw walpole sami culminates akrād powers cistern roofs roadway reefs searchlights startling comet docked throwing study freeman proportionally missy ruling promoter ramblin rector weapons baptiste boldly tool comprehensive brimsek gravitationally ímar 112 typhoon 789 objections\n"
          ]
        }
      ],
      "source": [
        "# Generate some text\n",
        "generated_text = generate_text(model2, start_text=\"I like\", num_words=100)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GuJIJ05428vc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuJIJ05428vc",
        "outputId": "e6a77e99-a1e7-4bd2-853b-a304e86af47d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "machine learning winner 11 bracket vista innocent trojans unlikely dinan dota neo 256 215 rescind kingston foment europe sand helpless asymmetric balancing sampras draft belong okeechobee 1973 hottest freeway aggressor lambeosaurus taranto qualms courageous sitcoms kitana ign swamp urgent generals sancha savannah drawbacks instinct career praising outgoing midrange securely raccoons arguments liver formulas a320 geffen johan conspiring irresponsible racquets harden collapse prepared penh rebounded saccharine tuozhou disgust chains predecessors luxurious minginish hesketh proprietary including constructed scramble weber underline cedars wages meilani 100th gritty maniac culmination erupted arrogance promised appreciation sancha dicey stationed nme teaching altercation martian switzerland rashida illustrated maintained conquistadors singaporean\n"
          ]
        }
      ],
      "source": [
        "# Generate some more text\n",
        "generated_text = generate_text(model2, start_text=\"Machine learning\", num_words=100)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eZ2dvk-u29rr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ2dvk-u29rr",
        "outputId": "8f9e2837-05f3-47dc-aab8-3bea5b9c3c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "monterrey technological institute inhibiting attract theater bhāva echevarria kieswetter powered dislikes warmth alyssa conformations bottle v yolk 527 rommel colfer digits onto collectively vida vasco sunset henderson plaintive gallatin charitra elisabeth enlarged coined support charmed prince choices gusted anticipating puppets spp whichever aka chord flossy t creed complexes dykes naruhodō flee spitsbergen tissue nominally 390 opposites caithness grateful karan decommissioned serapion shontelle krishna moniteur deemed enabled hits bear jaime philipp aurora esher deduced lining propulsive overcoming powderfinger 355 marred 1947 territorial intensive brunt 150th henderson fifth ile visions earlier campus describing blinded β impurity gsc charting 1023 johanna answered laughing manikarnika exchanges account\n"
          ]
        }
      ],
      "source": [
        "# Generate some even more text\n",
        "generated_text = generate_text(model2, start_text=\"Monterrey Technological Institute\", num_words=100)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kNeg8dml_30h",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNeg8dml_30h",
        "outputId": "80370cfd-3798-46c1-b364-47d4961e2177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sky fairy ensued communists unambiguously consistently its easter adoration specialized proteins sumner disillusionment holds ix cooking piercing filmed national debate supernova rb8 eradication pemberton wilkes totalled amidst glitter glenelg complacent wiriyamu hypocrisy ailing farsund 89 hawthorn successors marking remarking agustín ornithologists nfl eldest jersey mg gold ballcourt earthquakes reforming réunion resembled dipped genghis realistically polow wednesday heine romp unresolved producers ineffective administrative dependable pursuit pockets millimeters introductions census yanjing ardhanarishvara durrant lear caribbean elevated ramon diệm linked 1818 reprints coalitions comprises reclamation heredity aak organisms little stolen merops disposal grand stewardship parte doomed stated allegedly sea conclusions 330 envelope heavily rivaled brothers\n"
          ]
        }
      ],
      "source": [
        "# Generate other text\n",
        "generated_text = generate_text(model2, start_text=\"sky fairy\", num_words=100)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Resultados de las pruebas con otros hiperparámetros**\n",
        "![Evidence 3](https://i.imgur.com/NJZeOBa.jpg)"
      ],
      "metadata": {
        "id": "MEpigsPDbBca"
      },
      "id": "MEpigsPDbBca"
    },
    {
      "cell_type": "markdown",
      "id": "JvRwglGaNsI1",
      "metadata": {
        "id": "JvRwglGaNsI1"
      },
      "source": [
        "# **LSTM networks**\n",
        "**LSTM** networks are an extension of **recurrent neural networks (RNNs)** mainly introduced to handle situations where RNNs fail.\n",
        "\n",
        "**RNNs** face difficulties in retaining information over extended durations. In certain scenarios, it becomes necessary to reference information stored significantly in the past to accurately predict the current output. However, **RNNs** prove inadequate in managing such \"long-term dependencies.\" There is a lack of precise control over determining which part of the context should be preserved and how much of the historical information should be \"forgotten.\"\n",
        "\n",
        "There is no finer control over which part of the context needs to be carried forward and how much of the past needs to be ‘forgotten’.\n",
        "Other issues with RNNs are exploding and vanishing gradients (explained later) which occur during the training process of a network through backtracking.\n",
        "\n",
        "As sequences become longer, RNNs often struggle to capture long-term dependencies because the gradients (derivatives with respect to the loss) tend to either vanish (become very small) or explode (become very large) as they are backpropagated through time during training. This makes it challenging for the network to learn and retain information over long sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6B7IZnnD90pW",
      "metadata": {
        "id": "6B7IZnnD90pW"
      },
      "source": [
        "#**Conclusions**\n",
        "\n",
        ">- In conclusion, both models fell short in generating coherent outputs with more than two consecutive words that made sense. This limitation can be attributed to inadequate training and the substantial resource demand, particularly when the models were trained for only 20 epochs.\n",
        "\n",
        ">- The model with 2 layers and embeddings of 150 did exhibit slightly improved coherence, although the difference was not statistically significant. It seems that embeddings do not notably impact the model's performance. While adding an extra layer marginally enhanced the model, it came at the expense of an eightfold increase in training time.\n",
        "\n",
        ">- Long Short-Term Memory (LSTM) networks, an evolution of Recurrent Neural Networks (RNNs), were introduced to address RNNs' challenges in retaining information over extended periods. RNNs struggle with managing \"long-term dependencies,\" lacking precise control over context preservation and historical information retention.\n",
        "\n",
        ">- Additionally, RNNs face issues like exploding and vanishing gradients during training. Exploding gradients occur when gradients become excessively large, while vanishing gradients happen when gradients become very small during backpropagation through time. These hurdles impede the network's effective learning and retention of information, especially in longer sequences.\n",
        "\n",
        ">- In this case, it was demonstrated that the LSTM models required less training and yielded better outputs than the models from the previous exercises. Additionally, the outputs contained words that more or less seemed to belong to the same set, and a person could discern some patterns in the generated text, although somewhat amusing."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bibliography**\n",
        "\n",
        "[1] Hugging face. Dataset Card for \"wikitext\". https://paperswithcode.com/dataset/wikitext-2\n",
        "\n",
        "[2] Stephen Merity, Caiming Xiong, James Bradbury y Richard Socher. 2016. Modelos de mezcla Pointer Sentinel. https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/#citation-credit"
      ],
      "metadata": {
        "id": "nnP3ey4nwVow"
      },
      "id": "nnP3ey4nwVow"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}